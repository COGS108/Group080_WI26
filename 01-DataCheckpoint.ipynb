{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rubric\n",
        "\n",
        "Instructions: DELETE this cell before you submit via a `git push` to your repo before deadline. This cell is for your reference only and is not needed in your report. \n",
        "\n",
        "Scoring: Out of 10 points\n",
        "\n",
        "- Each Developing  => -2 pts\n",
        "- Each Unsatisfactory/Missing => -4 pts\n",
        "  - until the score is \n",
        "\n",
        "If students address the detailed feedback in a future checkpoint they will earn these points back\n",
        "\n",
        "\n",
        "|                  | Unsatisfactory                                                                                                                                                                                                    | Developing                                                                                                                                                                                              | Proficient                                     | Excellent                                                                                                                              |\n",
        "|------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| Data relevance   | Did not have data relevant to their question. Or the datasets don't work together because there is no way to line them up against each other. If there are multiple datasets, most of them have this trouble | Data was only tangentially relevant to the question or a bad proxy for the question. If there are multiple datasets, some of them may be irrelevant or can't be easily combined.                       | All data sources are relevant to the question. | Multiple data sources for each aspect of the project. It's clear how the data supports the needs of the project.                         |\n",
        "| Data description | Dataset or its cleaning procedures are not described. If there are multiple datasets, most have this trouble                                                                                              | Data was not fully described. If there are multiple datasets, some of them are not fully described                                                                                                      | Data was fully described                       | The details of the data descriptions and perhaps some very basic EDA also make it clear how the data supports the needs of the project. |\n",
        "| Data wrangling   | Did not obtain data. They did not clean/tidy the data they obtained.  If there are multiple datasets, most have this trouble                                                                                 | Data was partially cleaned or tidied. Perhaps you struggled to verify that the data was clean because they did not present it well. If there are multiple datasets, some have this trouble | The data is cleaned and tidied.                | The data is spotless and they used tools to visualize the data cleanliness and you were convinced at first glance                      |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# COGS 108 - Data Checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Authors\n",
        "\n",
        "**Ali Ahmed**: Background research, Writing - original draft\n",
        "\n",
        "**Rodayna Alnaggar**: Background research, Data curation\n",
        "\n",
        "**Tessa Kibbe**: Conceptualization, Writing - review & editing\n",
        "\n",
        "**Sabine A Sanchez**: Data curation, Methodology\n",
        "\n",
        "**Maanav R Singh**: Project administration, Writing - review & editing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Research Question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Has the introduction of Google AI Overviews in 2024 led to a statistically significant shift in the proportions of search behaviors (organic clicks vs. zero-click searches) for informational queries between 2023 and 2025?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Background and Prior Work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The way we search for information online is going through a huge change right now. For about 30 years, Google and other search engines worked pretty simply: you typed in a question, and they gave you a list of websites to click on. This \"Ten Blue Links\" model meant that users had to do the work of clicking through different sites, reading them, and figuring out what was true on their own. But now, with AI being added to search engines, things are different. Instead of just showing you where to find answers, search engines are now trying to give you the answer directly. Researchers call this shift moving from \"Information Retrieval\" to \"Generative Information Retrieval.\"\n",
        "\n",
        "This change didn't happen overnight, it built up over several years. In 2015, Google started using RankBrain; in 2019 they added BERT; in 2023 they launched the Search Generative Experience (SGE), which became \"AI Overviews\" in 2024. This was the first time Google actually showed AI-generated text at the top of search results instead of just links.\n",
        "\n",
        "One major consequence is what researchers call the \"Great Decoupling\"—more people are searching than ever, but fewer people are actually clicking through to websites. Industry studies (e.g., Seer Interactive, Ahrefs) report that AI Overviews reduce organic click-through rates by roughly 50–60%. We are using Wikipedia Clickstream and Google Trends to test whether the introduction of AI Overviews is associated with a measurable shift in click-through behavior for informational queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hypothesis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We hypothesize that the rollout of Google AI Overviews in 2024 has caused a statistically significant shift in search behavior for informational queries, characterized by an increase in zero-click search rates and a corresponding decrease in organic click-through rates. We expect to see this change in proportions specifically in informational queries compared to navigational and transactional queries because they seek to find answers rather than destinations.\n",
        "\n",
        "In addition, we hypothesize that the impact of AI Overviews on informational search behavior increased zero-click searches in a nonlinear pattern: modest changes immediately after rollout (lagged adoption), followed by a rapid increase as users grew more accustomed to relying on AI-generated summaries. This prediction is based on industry research showing that AI Overviews reduce organic clicks by 50–60% and the broader trend toward zero-click searches, suggesting users are increasingly accepting AI-generated answers without clicking through to verify information from original sources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data overview\n",
        "\n",
        "We use two datasets to proxy zero-click behavior: Wikipedia Clickstream (clicks from search to Wikipedia) and Google Trends (search interest over time). If search interest stays stable but clicks from Google to Wikipedia drop after AI Overviews, that gap supports an increase in zero-click behavior.\n",
        "\n",
        "- **Dataset #1 – Wikipedia Clickstream**\n",
        "  - **Dataset Name:** Wikimedia Clickstream (English Wikipedia, monthly)\n",
        "  - **Link:** https://dumps.wikimedia.org/other/clickstream/\n",
        "  - **Observations:** One row per (referrer, destination article) pair per month; millions of pairs per monthly file.\n",
        "  - **Variables:** `prev` (referrer: e.g. \"other-search\" for search engines, or article title), `curr` (destination article), `type` (link/external/other), `n` (count of that transition).\n",
        "  - **Relevance:** Filtering `prev = 'other-search'` (and/or search-related referrers) gives monthly counts of clicks from search engines to Wikipedia articles. A decline in these counts after mid-2024, for informational articles, is consistent with more zero-click behavior.\n",
        "  - **Shortcomings:** Aggregated; does not distinguish Google from other search engines; only captures traffic to Wikipedia, not to other sites; (referrer, resource) pairs with ≤10 observations are excluded.\n",
        "\n",
        "- **Dataset #2 – Google Trends**\n",
        "  - **Dataset Name:** Google Trends (interest over time)\n",
        "  - **Link:** https://trends.google.com/ (or via `pytrends` in Python)\n",
        "  - **Observations:** One row per date (or week) per keyword.\n",
        "  - **Variables:** `date`, keyword (search term), `interest_over_time` (0–100, relative popularity).\n",
        "  - **Relevance:** Shows whether people kept searching for the same topics (e.g. \"Quantum mechanics\", \"Climate change\") across 2023–2025. Stable or rising interest with falling Wikipedia clickstream counts would support that answers are being consumed in-search (zero-click).\n",
        "  - **Shortcomings:** Relative scale (0–100), not absolute search volume; possible sampling/API limits; geographic and category choices affect results.\n",
        "\n",
        "**Combining the datasets:** We will align both datasets by **time** (month) and **topic**. For each informational topic (e.g. a set of Wikipedia article titles and corresponding search terms), we will (1) compute monthly Wikipedia clickstream counts from search to those articles, and (2) get monthly Google Trends interest for the matching keywords. We will then compare pre– vs post–AI Overviews periods (e.g. 2023 vs 2024–2025) and test whether the ratio of clicks-to-Wikipedia to search-interest declines, which would be consistent with a shift toward zero-click behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run this code every time when you're actively developing modules in .py files.  It's not needed if you aren't making modules\n",
        "#\n",
        "## this code is necessary for making sure that any modules we load are updated here \n",
        "## when their source code .py files are modified\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Wikipedia Clickstream (Dataset #1)\n",
        "\n",
        "**Metrics and units:** Each row is a (referrer, resource) pair with count `n`. **prev**: referrer—either an article title (internal link) or a fixed code (e.g. `other-search` for external search engines, `other-external` for other external sites, `other-empty` for no referrer). **curr**: destination Wikipedia article (main namespace). **type**: `link` (prev links to curr), `external` (prev is external), or `other`. **n**: integer count of that (prev, curr) transition in that month. Pairs with 10 or fewer observations are excluded in the source. Counts are aggregated over desktop, mobile web, and mobile app.\n",
        "\n",
        "**Relevance to the project:** Filtering rows where `prev == 'other-search'` gives monthly counts of clicks from search engines to Wikipedia articles. Summing `n` over selected informational articles (or over all articles) yields a proxy for “organic clicks from search to Wikipedia.” A drop in this total after AI Overviews (2024) would be consistent with more zero-click behavior.\n",
        "\n",
        "**Concerns:** Data is aggregated and anonymized; we cannot separate Google from other search engines. Only Wikipedia traffic is observed, so we are measuring one slice of organic clicks. Low-count pairs are dropped at source. Referrer mapping may change over time (e.g. “other-search” aggregates all search engines).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data/00-raw/clickstream-enwiki-2023-06.tsv.gz ...\n",
            "Loading data/00-raw/clickstream-enwiki-2024-06.tsv.gz ...\n",
            "Loading data/00-raw/clickstream-enwiki-2025-01.tsv.gz ...\n",
            "Shape: (104239331, 5)\n",
            "Columns: ['prev', 'curr', 'type', 'n', 'month']\n",
            "\n",
            "First few rows:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prev</th>\n",
              "      <th>curr</th>\n",
              "      <th>type</th>\n",
              "      <th>n</th>\n",
              "      <th>month</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>other-empty</td>\n",
              "      <td>Kensey_Johns_Jr.</td>\n",
              "      <td>external</td>\n",
              "      <td>32.0</td>\n",
              "      <td>2023-06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>other-empty</td>\n",
              "      <td>Tengah_Islands</td>\n",
              "      <td>external</td>\n",
              "      <td>14.0</td>\n",
              "      <td>2023-06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>other-search</td>\n",
              "      <td>Kensey_Johns_Jr.</td>\n",
              "      <td>external</td>\n",
              "      <td>17.0</td>\n",
              "      <td>2023-06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>other-search</td>\n",
              "      <td>Tengah_Islands</td>\n",
              "      <td>external</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2023-06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>List_of_number-one_hits_of_1973_(Mexico)</td>\n",
              "      <td>List_of_number-one_hits_of_1974_(Mexico)</td>\n",
              "      <td>link</td>\n",
              "      <td>16.0</td>\n",
              "      <td>2023-06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>other-empty</td>\n",
              "      <td>1957–58_British_Home_Championship</td>\n",
              "      <td>external</td>\n",
              "      <td>27.0</td>\n",
              "      <td>2023-06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>other-empty</td>\n",
              "      <td>List_of_number-one_hits_of_1974_(Mexico)</td>\n",
              "      <td>external</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2023-06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>British_Home_Championship</td>\n",
              "      <td>1957–58_British_Home_Championship</td>\n",
              "      <td>link</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2023-06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>other-search</td>\n",
              "      <td>List_of_number-one_hits_of_1974_(Mexico)</td>\n",
              "      <td>external</td>\n",
              "      <td>43.0</td>\n",
              "      <td>2023-06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>other-empty</td>\n",
              "      <td>Sensory_history</td>\n",
              "      <td>external</td>\n",
              "      <td>156.0</td>\n",
              "      <td>2023-06</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       prev  \\\n",
              "0                               other-empty   \n",
              "1                               other-empty   \n",
              "2                              other-search   \n",
              "3                              other-search   \n",
              "4  List_of_number-one_hits_of_1973_(Mexico)   \n",
              "5                               other-empty   \n",
              "6                               other-empty   \n",
              "7                 British_Home_Championship   \n",
              "8                              other-search   \n",
              "9                               other-empty   \n",
              "\n",
              "                                       curr      type      n    month  \n",
              "0                          Kensey_Johns_Jr.  external   32.0  2023-06  \n",
              "1                            Tengah_Islands  external   14.0  2023-06  \n",
              "2                          Kensey_Johns_Jr.  external   17.0  2023-06  \n",
              "3                            Tengah_Islands  external   10.0  2023-06  \n",
              "4  List_of_number-one_hits_of_1974_(Mexico)      link   16.0  2023-06  \n",
              "5         1957–58_British_Home_Championship  external   27.0  2023-06  \n",
              "6  List_of_number-one_hits_of_1974_(Mexico)  external   10.0  2023-06  \n",
              "7         1957–58_British_Home_Championship      link   10.0  2023-06  \n",
              "8  List_of_number-one_hits_of_1974_(Mexico)  external   43.0  2023-06  \n",
              "9                           Sensory_history  external  156.0  2023-06  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Value counts for 'prev' (top referrers):\n",
            "prev\n",
            "other-empty                                                      12766080\n",
            "other-search                                                     11725128\n",
            "other-internal                                                    4467687\n",
            "other-external                                                    1641561\n",
            "Main_Page                                                          806020\n",
            "other-other                                                        515407\n",
            "Wikipedia                                                           25684\n",
            "Wiki                                                                 9139\n",
            "Deaths_in_2023                                                       5956\n",
            "United_States                                                        5244\n",
            "Deaths_in_2024                                                       5147\n",
            "List_of_Korean_dramas                                                5108\n",
            "List_of_one-hit_wonders_in_the_United_States                         4947\n",
            "List_of_accidents_and_incidents_involving_commercial_aircraft        4837\n",
            "List_of_American_film_actresses                                      4809\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Ensure directories exist\n",
        "os.makedirs('data/01-interim', exist_ok=True)\n",
        "os.makedirs('data/02-processed', exist_ok=True)\n",
        "\n",
        "# Load all Wikipedia Clickstream monthly files from 00-raw\n",
        "raw_dir = 'data/00-raw'\n",
        "pattern = os.path.join(raw_dir, 'clickstream-enwiki-*.tsv.gz')\n",
        "files = sorted(glob.glob(pattern))\n",
        "\n",
        "if not files:\n",
        "    raise FileNotFoundError(\"No clickstream files found in data/00-raw/. Run the setup cell above to download them.\")\n",
        "\n",
        "# Load and parse each monthly file; add month column\n",
        "dfs = []\n",
        "for path in files:\n",
        "    # e.g. clickstream-enwiki-2024-06.tsv.gz -> 2024-06\n",
        "    print(f\"Loading {path} ...\")\n",
        "    basename = os.path.basename(path)\n",
        "    month = basename.replace('clickstream-enwiki-', '').replace('.tsv.gz', '')\n",
        "    df = pd.read_csv(\n",
        "        path, sep='\\t', compression='gzip',\n",
        "        header=None, names=['prev', 'curr', 'type', 'n'],\n",
        "        on_bad_lines='skip'\n",
        "    )\n",
        "    df['month'] = month\n",
        "    dfs.append(df)\n",
        "\n",
        "clickstream_raw = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "print(\"Shape:\", clickstream_raw.shape)\n",
        "print(\"Columns:\", clickstream_raw.columns.tolist())\n",
        "print(\"\\nFirst few rows:\")\n",
        "display(clickstream_raw.head(10))\n",
        "print(\"\\nValue counts for 'prev' (top referrers):\")\n",
        "print(clickstream_raw['prev'].value_counts().head(15))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing per column:\n",
            "prev     108\n",
            "curr     172\n",
            "type       0\n",
            "n         24\n",
            "month      0\n",
            "dtype: int64\n",
            "\n",
            "Rows with prev='other-search': 11725128\n",
            "\n",
            "Total clicks from search by month:\n",
            "     month  clicks_from_search\n",
            "0  2023-06        3.123554e+09\n",
            "1  2024-06        2.955755e+09\n",
            "2  2025-01        3.319773e+09\n",
            "\n",
            "Saved to data/02-processed/\n"
          ]
        }
      ],
      "source": [
        "# Missingness\n",
        "print(\"Missing per column:\")\n",
        "print(clickstream_raw.isnull().sum())\n",
        "# Clickstream dumps are complete for (prev, curr) pairs; no NA expected in prev, curr, n.\n",
        "\n",
        "# Restrict to search-originated traffic: prev == 'other-search' (external search engines)\n",
        "clickstream_search = clickstream_raw[clickstream_raw['prev'] == 'other-search'].copy()\n",
        "print(\"\\nRows with prev='other-search':\", len(clickstream_search))\n",
        "\n",
        "# Tidy: one row per (month, curr) with total clicks from search to that article\n",
        "clickstream_by_month = (\n",
        "    clickstream_search.groupby(['month', 'curr'], as_index=False)['n']\n",
        "    .sum()\n",
        "    .rename(columns={'n': 'clicks_from_search'})\n",
        ")\n",
        "# Total clicks from search per month (for trend analysis)\n",
        "monthly_totals = clickstream_by_month.groupby('month', as_index=False)['clicks_from_search'].sum()\n",
        "print(\"\\nTotal clicks from search by month:\")\n",
        "print(monthly_totals)\n",
        "\n",
        "# Save processed clickstream: search-only, by month and article\n",
        "clickstream_by_month.to_csv('data/02-processed/wikipedia_clickstream_search_by_month_article.csv', index=False)\n",
        "monthly_totals.to_csv('data/02-processed/wikipedia_clickstream_search_monthly_totals.csv', index=False)\n",
        "print(\"\\nSaved to data/02-processed/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Google Trends (Dataset #2)\n",
        "\n",
        "**Metrics and units:** Google Trends provides **interest over time**: a 0–100 index of relative search popularity for a given keyword in a given time window (no absolute volume). We use **date** (or weekly buckets) and **interest** (0–100) per keyword. This tells us whether people were still searching for the same topics across 2023–2025.\n",
        "\n",
        "**Relevance:** If Trends interest stays stable or rises while Wikipedia clickstream counts from search drop, that supports the hypothesis that more searches are “zero-click” (answers consumed in-search rather than by clicking through to Wikipedia).\n",
        "\n",
        "**Concerns:** Relative scale only; geographic and category filters affect results; API/rate limits may apply when using `pytrends`; data is sampled.\n",
        "\n",
        "**Obtaining the data:** We use the `pytrends` library to fetch interest over time for chosen keywords (e.g. \"Quantum mechanics\", \"Climate change\"). Below we load or fetch a small example and tidy it; for the full analysis we will fetch multiple keywords and align by month with the clickstream data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape: (218, 4)\n",
            "        date            keyword  interest    month\n",
            "0 2023-01-01  Quantum mechanics         0  2023-01\n",
            "1 2023-01-08  Quantum mechanics         0  2023-01\n",
            "2 2023-01-15  Quantum mechanics         0  2023-01\n",
            "3 2023-01-22  Quantum mechanics         0  2023-01\n",
            "4 2023-01-29  Quantum mechanics         0  2023-01\n",
            "5 2023-02-05  Quantum mechanics         0  2023-02\n",
            "6 2023-02-12  Quantum mechanics         0  2023-02\n",
            "7 2023-02-19  Quantum mechanics         0  2023-02\n",
            "8 2023-02-26  Quantum mechanics         0  2023-02\n",
            "9 2023-03-05  Quantum mechanics         0  2023-03\n",
            "\n",
            "Saved to data/02-processed/google_trends_interest_long.csv\n"
          ]
        }
      ],
      "source": [
        "# Google Trends: fetch via pytrends (install with: pip install pytrends)\n",
        "# If pytrends is not available, we can load a pre-downloaded CSV from 00-raw instead.\n",
        "\n",
        "try:\n",
        "    from pytrends.request import TrendReq\n",
        "    pytrends_available = True\n",
        "except ImportError:\n",
        "    pytrends_available = False\n",
        "    print(\"pytrends not installed. Install with: pip install pytrends\")\n",
        "    print(\"Alternatively, export CSVs from https://trends.google.com/ and load from data/00-raw/\")\n",
        "\n",
        "if pytrends_available:\n",
        "    pt = TrendReq(hl='en-US', tz=360)\n",
        "    # Example: interest over time for a few informational keywords (2023-01-01 to 2025-02-01)\n",
        "    keywords = [\"Quantum mechanics\", \"Climate change\"]\n",
        "    pt.build_payload(keywords, timeframe='2023-01-01 2025-02-01', geo='')\n",
        "    trends_df = pt.interest_over_time()\n",
        "    if 'isPartial' in trends_df.columns:\n",
        "        trends_df = trends_df.drop(columns=['isPartial'])\n",
        "    trends_df = trends_df.reset_index()\n",
        "    trends_df = trends_df.rename(columns={'date': 'date'})\n",
        "    # Tidy: long form (date, keyword, interest)\n",
        "    trends_long = trends_df.melt(id_vars=['date'], var_name='keyword', value_name='interest')\n",
        "    trends_long['month'] = trends_long['date'].dt.to_period('M').astype(str)\n",
        "    print(\"Shape:\", trends_long.shape)\n",
        "    print(trends_long.head(10))\n",
        "    # Save processed Trends data\n",
        "    trends_long.to_csv('data/02-processed/google_trends_interest_long.csv', index=False)\n",
        "    print(\"\\nSaved to data/02-processed/google_trends_interest_long.csv\")\n",
        "else:\n",
        "    # Placeholder: create minimal structure so rest of notebook can run\n",
        "    import pandas as pd\n",
        "    trends_long = pd.DataFrame({'month': [], 'keyword': [], 'interest': []})\n",
        "    print(\"Google Trends data will be loaded from CSV or fetched once pytrends is installed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combining the two datasets\n",
        "\n",
        "We align Wikipedia Clickstream and Google Trends by **month**. For each month we have (1) total (or topic-specific) clicks from search to Wikipedia from the clickstream data, and (2) average or total interest per keyword from Trends. We can then compute a simple ratio or normalized metric (e.g. clicks per unit of interest) and compare pre–AI Overviews (e.g. 2023) vs post–AI Overviews (2024–2025). Below we load the processed files and join on `month` for illustration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined dataset (by month):\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>month</th>\n",
              "      <th>wiki_clicks_from_search</th>\n",
              "      <th>interest</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2023-01</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2023-02</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2023-03</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2023-04</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8.300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2023-05</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2023-06</td>\n",
              "      <td>3.123554e+09</td>\n",
              "      <td>1.750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2023-07</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2023-08</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2023-09</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2023-10</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2023-11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2023-12</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2024-01</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2024-02</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2024-03</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2024-04</td>\n",
              "      <td>NaN</td>\n",
              "      <td>14.500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2024-05</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2024-06</td>\n",
              "      <td>2.955755e+09</td>\n",
              "      <td>1.600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2024-07</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2024-08</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>2024-09</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>2024-10</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>2024-11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>2024-12</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>2025-01</td>\n",
              "      <td>3.319773e+09</td>\n",
              "      <td>2.125</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      month  wiki_clicks_from_search  interest\n",
              "0   2023-01                      NaN     1.900\n",
              "1   2023-02                      NaN     2.125\n",
              "2   2023-03                      NaN     2.500\n",
              "3   2023-04                      NaN     8.300\n",
              "4   2023-05                      NaN     2.375\n",
              "5   2023-06             3.123554e+09     1.750\n",
              "6   2023-07                      NaN     1.600\n",
              "7   2023-08                      NaN     1.625\n",
              "8   2023-09                      NaN     2.000\n",
              "9   2023-10                      NaN     2.000\n",
              "10  2023-11                      NaN     2.000\n",
              "11  2023-12                      NaN     1.600\n",
              "12  2024-01                      NaN     1.875\n",
              "13  2024-02                      NaN     2.125\n",
              "14  2024-03                      NaN     2.200\n",
              "15  2024-04                      NaN    14.500\n",
              "16  2024-05                      NaN     2.375\n",
              "17  2024-06             2.955755e+09     1.600\n",
              "18  2024-07                      NaN     1.500\n",
              "19  2024-08                      NaN     1.500\n",
              "20  2024-09                      NaN     1.900\n",
              "21  2024-10                      NaN     2.125\n",
              "22  2024-11                      NaN     2.375\n",
              "23  2024-12                      NaN     1.600\n",
              "24  2025-01             3.319773e+09     2.125"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load processed data and align by month\n",
        "monthly_totals = pd.read_csv('data/02-processed/wikipedia_clickstream_search_monthly_totals.csv')\n",
        "monthly_totals = monthly_totals.rename(columns={'clicks_from_search': 'wiki_clicks_from_search'})\n",
        "\n",
        "# If we have Trends data, average interest by month and join\n",
        "trends_path = 'data/02-processed/google_trends_interest_long.csv'\n",
        "if os.path.exists(trends_path) and os.path.getsize(trends_path) > 0:\n",
        "    trends = pd.read_csv(trends_path)\n",
        "    trends['date'] = pd.to_datetime(trends['date'])\n",
        "    trends['month'] = trends['date'].dt.to_period('M').astype(str)\n",
        "    trends_monthly = trends.groupby('month', as_index=False)['interest'].mean()\n",
        "    combined = monthly_totals.merge(trends_monthly, on='month', how='outer')\n",
        "    print(\"Combined dataset (by month):\")\n",
        "    display(combined)\n",
        "else:\n",
        "    print(\"Wikipedia monthly totals (Trends will be joined when available):\")\n",
        "    display(monthly_totals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ethics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For each item: put an X if considered; if relevant, add a short paragraph. See [Deon checklist examples](https://deon.drivendata.org/examples/).\n",
        "\n",
        "**A. Data Collection**\n",
        "- [X] **A.1 Informed consent:** We use clickstream and SEO-style data from platforms (e.g. Wikimedia, Google Trends) that publish aggregated, anonymized data. We only use sources that state consent/anonymization practices.\n",
        "- [X] **A.2 Collection bias:** We acknowledge bias (e.g. tech-savvy users, desktop vs mobile). We stratify by device where possible and state sampling limitations.\n",
        "- [X] **A.3 Limit PII exposure:** We use only aggregated, anonymized data (no individual search histories or PII).\n",
        "- [ ] **A.4 Downstream bias mitigation:** Not applicable (no protected group status collected).\n",
        "\n",
        "**B. Data Storage**\n",
        "- [ ] **B.1 Data security:** Plan to protect data (e.g. access controls, no raw data in public repo if sensitive).\n",
        "- [ ] **B.2 Right to be forgotten:** N/A for aggregate public datasets.\n",
        "- [ ] **B.3 Data retention plan:** Plan to delete or archive after project if required.\n",
        "\n",
        "**C. Analysis**\n",
        "- [ ] **C.1 Missing perspectives:** We will consider stakeholder blindspots where relevant.\n",
        "- [X] **C.2 Dataset bias:** We address bias (e.g. commercial/English-language emphasis) by defining scope to informational queries and stating limitations.\n",
        "- [X] **C.3 Honest representation:** Visualizations and statistics will represent the data honestly.\n",
        "- [X] **C.4 Privacy in analysis:** No PII used or displayed.\n",
        "- [X] **C.5 Auditability:** Process documented in Jupyter notebooks; code in GitHub for reproducibility.\n",
        "\n",
        "**D. Modeling / E. Deployment:** Addressed in later checkpoints if we deploy models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Team Expectations "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Communication:** Primarily via iMessage group chat; meet via FaceTime or in person as needed. Respond within 24 hours; notify in advance if unable to attend a meeting.\n",
        "- **Equal contribution:** Each member contributes equally across research, coding, writing, and editing; we rotate responsibilities.\n",
        "- **Tone and respect:** Blunt but polite; use \"I statements\" when giving feedback; assume criticism is well-intentioned.\n",
        "- **Task management:** Use GitHub for tasks and deadlines; assign fairly by strengths and availability. If struggling, notify within 48 hours to redistribute.\n",
        "- **Decision making:** Majority vote for major decisions; for urgent decisions when someone is unresponsive, available members can proceed and update afterward.\n",
        "- **Accountability:** If someone is not meeting expectations, address directly (e.g. via text) with one week to improve and specific deliverables; redistribute tasks as needed.\n",
        "- **Deadlines:** Internal deadlines 2–3 days before official course deadlines for review and revision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Project Timeline Proposal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Special resources/training:** Statistical methods for pre/post AI Overviews comparison (e.g. t-tests, chi-square); clear visualizations for trends over time.\n",
        "\n",
        "| Meeting Date | Meeting Time | Completed Before Meeting | Discuss at Meeting |\n",
        "|--------------|--------------|--------------------------|-------------------|\n",
        "| 1/15 | 12 PM | NA | Determine best form of communication |\n",
        "| 1/23 | 10 AM | Brainstorm topics (All) | Brainstorm topics (All) |\n",
        "| 1/26 | 10 AM | Background research (Ali, Rodayna); Ethics draft (Sabine) | Ideal dataset(s) and ethics; draft proposal |\n",
        "| 2/4 | 10 AM | Edit, finalize, submit proposal (Maanav, Tessa); Search datasets (All) | Wrangling and analysis approaches; assign leads |\n",
        "| 2/4 | Before 11:59 PM | NA | **Turn in Project Proposal** |\n",
        "| 2/14 | 6 PM | Import & wrangle data (Maanav); Initial EDA with 3+ visualizations (Rodayna) | Review wrangling/EDA; analysis plan; data quality |\n",
        "| 2/14 | Before 11:59 PM | NA | **Turn in Data Checkpoint** |\n",
        "| 2/23 | 12 PM | Finalize wrangling/EDA (Maanav, Rodayna); Begin pre/post statistical analysis (Sabine, Tessa) | Edit analysis; preliminary results; check-in |\n",
        "| 3/4 | Before 11:59 PM | NA | **Turn in EDA Checkpoint** |\n",
        "| 3/9 | 12 PM | Finalize analysis and visualizations; Draft results/conclusion (Ali); Polish notebook (Maanav) | Review analysis; edit results/discussion; video outline |\n",
        "| 3/13 | 12 PM | Ethics updates (Sabine); Finalize written sections (Ali, Tessa); Record video segments (All) | Integrate video; final review |\n",
        "| 3/18 | Before 11:59 PM | NA | **Turn in Final Project & Video** |\n",
        "\n",
        "*Timeline updated for Data Checkpoint: data sources set (Wikipedia Clickstream + Google Trends); wrangling and combination by month in progress.*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
