{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubric\n",
    "\n",
    "Instructions: DELETE this cell before you submit via a `git push` to your repo before deadline. This cell is for your reference only and is not needed in your report. \n",
    "\n",
    "Scoring: Out of 10 points\n",
    "\n",
    "- Each Developing  => -2 pts\n",
    "- Each Unsatisfactory/Missing => -4 pts\n",
    "  - until the score is \n",
    "\n",
    "If students address the detailed feedback in a future checkpoint they will earn these points back\n",
    "\n",
    "\n",
    "|                  | Unsatisfactory                                                                                                                                                                                                    | Developing                                                                                                                                                                                              | Proficient                                     | Excellent                                                                                                                              |\n",
    "|------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Data relevance   | Did not have data relevant to their question. Or the datasets don't work together because there is no way to line them up against each other. If there are multiple datasets, most of them have this trouble | Data was only tangentially relevant to the question or a bad proxy for the question. If there are multiple datasets, some of them may be irrelevant or can't be easily combined.                       | All data sources are relevant to the question. | Multiple data sources for each aspect of the project. It's clear how the data supports the needs of the project.                         |\n",
    "| Data description | Dataset or its cleaning procedures are not described. If there are multiple datasets, most have this trouble                                                                                              | Data was not fully described. If there are multiple datasets, some of them are not fully described                                                                                                      | Data was fully described                       | The details of the data descriptions and perhaps some very basic EDA also make it clear how the data supports the needs of the project. |\n",
    "| Data wrangling   | Did not obtain data. They did not clean/tidy the data they obtained.  If there are multiple datasets, most have this trouble                                                                                 | Data was partially cleaned or tidied. Perhaps you struggled to verify that the data was clean because they did not present it well. If there are multiple datasets, some have this trouble | The data is cleaned and tidied.                | The data is spotless and they used tools to visualize the data cleanliness and you were convinced at first glance                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Data Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "**Ali Ahmed**: Background research, Writing - original draft\n",
    "\n",
    "**Rodayna Alnaggar**: Background research, Writing - review & editing\n",
    "\n",
    "**Tessa Kibbe**: Conceptualization, Writing - review & editing\n",
    "\n",
    "**Sabine A Sanchez**: Data curation, Methodology\n",
    "\n",
    "**Maanav R Singh**: Project administration, Data curation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did the introduction of Google AI Overviews in the U.S. in May 2024 lead to a statistically significant shift in the proportions of organic clicks vs. zero-click searches for informational queries (defined as queries beginning with \"what,\" \"how,\" \"when,\" \"where,\" or \"why\") when comparing the one-year period before before and after the rollout?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way we search for information online is going through a huge change right now. For about 30 years, Google and other search engines worked pretty simply: you typed in a question, and they gave you a list of websites to click on. This \"Ten Blue Links\" model meant that users had to do the work of clicking through different sites, reading them, and figuring out what was true on their own. But now, with AI being added to search engines, things are different. Instead of just showing you where to find answers, search engines are now trying to give you the answer directly. Researchers call this shift moving from \"Information Retrieval\" to \"Generative Information Retrieval.\"\n",
    "\n",
    "This change didn't happen overnight, it built up over several years. In 2015, Google started using RankBrain; in 2019 they added BERT; in 2023 they launched the Search Generative Experience (SGE), which became \"AI Overviews\" in 2024. This was the first time Google actually showed AI-generated text at the top of search results instead of just links.\n",
    "\n",
    "One major consequence is what researchers call the \"Great Decoupling\"—more people are searching than ever, but fewer people are actually clicking through to websites. Industry studies (e.g., Seer Interactive, Ahrefs) report that AI Overviews reduce organic click-through rates by roughly 50–60%. We are using Wikipedia Clickstream and Google Trends to test whether the introduction of AI Overviews is associated with a measurable shift in click-through behavior for informational queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hypothesize that the rollout of Google AI Overviews in 2024 has caused a statistically significant shift in search behavior for informational queries, characterized by an increase in zero-click search rates and a corresponding decrease in organic click-through rates. We expect to see this change in proportions specifically in informational queries compared to navigational and transactional queries because they seek to find answers rather than destinations.\n",
    "\n",
    "In addition, we hypothesize that the impact of AI Overviews on informational search behavior increased zero-click searches in a nonlinear pattern: modest changes immediately after rollout (lagged adoption), followed by a rapid increase as users grew more accustomed to relying on AI-generated summaries. This prediction is based on industry research showing that AI Overviews reduce organic clicks by 50–60% and the broader trend toward zero-click searches, suggesting users are increasingly accepting AI-generated answers without clicking through to verify information from original sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data overview\n",
    "\n",
    "We use two datasets to proxy zero-click behavior: Wikipedia Clickstream (clicks from search to Wikipedia) and Google Trends (search interest over time). If search interest stays stable but clicks from Google to Wikipedia drop after AI Overviews, that gap supports an increase in zero-click behavior.\n",
    "\n",
    "- **Dataset #1 – Wikipedia Clickstream**\n",
    "  - **Dataset Name:** Wikimedia Clickstream (English Wikipedia, monthly)\n",
    "  - **Link:** https://dumps.wikimedia.org/other/clickstream/\n",
    "  - **Observations:** One row per (referrer, destination article) pair per month; millions of pairs per monthly file.\n",
    "  - **Variables:** `prev` (referrer: e.g. \"other-search\" for search engines, or article title), `curr` (destination article), `type` (link/external/other), `n` (count of that transition).\n",
    "  - **Relevance:** Filtering `prev = 'other-search'` (and/or search-related referrers) gives monthly counts of clicks from search engines to Wikipedia articles. A decline in these counts after mid-2024, for informational articles, is consistent with more zero-click behavior.\n",
    "  - **Shortcomings:** Aggregated; does not distinguish Google from other search engines; only captures traffic to Wikipedia, not to other sites; (referrer, resource) pairs with ≤10 observations are excluded.\n",
    "\n",
    "- **Dataset #2 – Google Trends**\n",
    "  - **Dataset Name:** Google Trends (interest over time)\n",
    "  - **Link:** https://trends.google.com/ (or via `pytrends` in Python)\n",
    "  - **Observations:** One row per date (or week) per keyword.\n",
    "  - **Variables:** `date`, keyword (search term), `interest_over_time` (0–100, relative popularity).\n",
    "  - **Relevance:** Shows whether people kept searching for the same topics (e.g. \"Quantum mechanics\", \"Climate change\") across 2023–2025. Stable or rising interest with falling Wikipedia clickstream counts would support that answers are being consumed in-search (zero-click).\n",
    "  - **Shortcomings:** Relative scale (0–100), not absolute search volume; possible sampling/API limits; geographic and category choices affect results.\n",
    "\n",
    "**Combining the datasets:** We will align both datasets by **time** (month) and **topic**. For each informational topic (e.g. a set of Wikipedia article titles and corresponding search terms), we will (1) compute monthly Wikipedia clickstream counts from search to those articles, and (2) get monthly Google Trends interest for the matching keywords. We will then compare pre– vs post–AI Overviews periods (e.g. 2023 vs 2024–2025) and test whether the ratio of clicks-to-Wikipedia to search-interest declines, which would be consistent with a shift toward zero-click behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code every time when you're actively developing modules in .py files.  It's not needed if you aren't making modules\n",
    "#\n",
    "## this code is necessary for making sure that any modules we load are updated here \n",
    "## when their source code .py files are modified\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: download raw data from source to data/00-raw/\n",
    "# Only needs to be run once after cloning the repo.\n",
    "\n",
    "import sys, os\n",
    "sys.path.append('./modules')\n",
    "import get_data\n",
    "\n",
    "os.makedirs('data/00-raw', exist_ok=True)\n",
    "\n",
    "datafiles = [\n",
    "    {'url': 'https://dumps.wikimedia.org/other/clickstream/2023-06/clickstream-enwiki-2023-06.tsv.gz', 'filename': 'clickstream-enwiki-2023-06.tsv.gz'},\n",
    "    {'url': 'https://dumps.wikimedia.org/other/clickstream/2024-06/clickstream-enwiki-2024-06.tsv.gz', 'filename': 'clickstream-enwiki-2024-06.tsv.gz'},\n",
    "    {'url': 'https://dumps.wikimedia.org/other/clickstream/2025-01/clickstream-enwiki-2025-01.tsv.gz', 'filename': 'clickstream-enwiki-2025-01.tsv.gz'},\n",
    "]\n",
    "get_data.get_raw(datafiles, destination_directory='data/00-raw/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia Clickstream (Dataset #1)\n",
    "\n",
    "**Metrics and units:** Each row is a (referrer, resource) pair with count `n`. **prev**: referrer—either an article title (internal link) or a fixed code (e.g. `other-search` for external search engines, `other-external` for other external sites, `other-empty` for no referrer). **curr**: destination Wikipedia article (main namespace). **type**: `link` (prev links to curr), `external` (prev is external), or `other`. **n**: integer count of that (prev, curr) transition in that month. Pairs with 10 or fewer observations are excluded in the source. Counts are aggregated over desktop, mobile web, and mobile app.\n",
    "\n",
    "**Relevance to the project:** Filtering rows where `prev == 'other-search'` gives monthly counts of clicks from search engines to Wikipedia articles. Summing `n` over selected informational articles (or over all articles) yields a proxy for “organic clicks from search to Wikipedia.” A drop in this total after AI Overviews (2024) would be consistent with more zero-click behavior.\n",
    "\n",
    "**Concerns:** Data is aggregated and anonymized; we cannot separate Google from other search engines. Only Wikipedia traffic is observed, so we are measuring one slice of organic clicks. Low-count pairs are dropped at source. Referrer mapping may change over time (e.g. “other-search” aggregates all search engines).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data/00-raw/clickstream-enwiki-2023-06.tsv.gz ...\n",
      "Loading data/00-raw/clickstream-enwiki-2024-06.tsv.gz ...\n",
      "Loading data/00-raw/clickstream-enwiki-2025-01.tsv.gz ...\n",
      "Shape: (104239331, 5)\n",
      "Columns: ['prev', 'curr', 'type', 'n', 'month']\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prev</th>\n",
       "      <th>curr</th>\n",
       "      <th>type</th>\n",
       "      <th>n</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>other-empty</td>\n",
       "      <td>Kensey_Johns_Jr.</td>\n",
       "      <td>external</td>\n",
       "      <td>32.0</td>\n",
       "      <td>2023-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>other-empty</td>\n",
       "      <td>Tengah_Islands</td>\n",
       "      <td>external</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2023-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>other-search</td>\n",
       "      <td>Kensey_Johns_Jr.</td>\n",
       "      <td>external</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2023-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>other-search</td>\n",
       "      <td>Tengah_Islands</td>\n",
       "      <td>external</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2023-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>List_of_number-one_hits_of_1973_(Mexico)</td>\n",
       "      <td>List_of_number-one_hits_of_1974_(Mexico)</td>\n",
       "      <td>link</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2023-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>other-empty</td>\n",
       "      <td>1957–58_British_Home_Championship</td>\n",
       "      <td>external</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2023-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>other-empty</td>\n",
       "      <td>List_of_number-one_hits_of_1974_(Mexico)</td>\n",
       "      <td>external</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2023-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>British_Home_Championship</td>\n",
       "      <td>1957–58_British_Home_Championship</td>\n",
       "      <td>link</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2023-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>other-search</td>\n",
       "      <td>List_of_number-one_hits_of_1974_(Mexico)</td>\n",
       "      <td>external</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2023-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>other-empty</td>\n",
       "      <td>Sensory_history</td>\n",
       "      <td>external</td>\n",
       "      <td>156.0</td>\n",
       "      <td>2023-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       prev  \\\n",
       "0                               other-empty   \n",
       "1                               other-empty   \n",
       "2                              other-search   \n",
       "3                              other-search   \n",
       "4  List_of_number-one_hits_of_1973_(Mexico)   \n",
       "5                               other-empty   \n",
       "6                               other-empty   \n",
       "7                 British_Home_Championship   \n",
       "8                              other-search   \n",
       "9                               other-empty   \n",
       "\n",
       "                                       curr      type      n    month  \n",
       "0                          Kensey_Johns_Jr.  external   32.0  2023-06  \n",
       "1                            Tengah_Islands  external   14.0  2023-06  \n",
       "2                          Kensey_Johns_Jr.  external   17.0  2023-06  \n",
       "3                            Tengah_Islands  external   10.0  2023-06  \n",
       "4  List_of_number-one_hits_of_1974_(Mexico)      link   16.0  2023-06  \n",
       "5         1957–58_British_Home_Championship  external   27.0  2023-06  \n",
       "6  List_of_number-one_hits_of_1974_(Mexico)  external   10.0  2023-06  \n",
       "7         1957–58_British_Home_Championship      link   10.0  2023-06  \n",
       "8  List_of_number-one_hits_of_1974_(Mexico)  external   43.0  2023-06  \n",
       "9                           Sensory_history  external  156.0  2023-06  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Value counts for 'prev' (top referrers):\n",
      "prev\n",
      "other-empty                                                      12766080\n",
      "other-search                                                     11725128\n",
      "other-internal                                                    4467687\n",
      "other-external                                                    1641561\n",
      "Main_Page                                                          806020\n",
      "other-other                                                        515407\n",
      "Wikipedia                                                           25684\n",
      "Wiki                                                                 9139\n",
      "Deaths_in_2023                                                       5956\n",
      "United_States                                                        5244\n",
      "Deaths_in_2024                                                       5147\n",
      "List_of_Korean_dramas                                                5108\n",
      "List_of_one-hit_wonders_in_the_United_States                         4947\n",
      "List_of_accidents_and_incidents_involving_commercial_aircraft        4837\n",
      "List_of_American_film_actresses                                      4809\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs('data/01-interim', exist_ok=True)\n",
    "os.makedirs('data/02-processed', exist_ok=True)\n",
    "\n",
    "# Load all Wikipedia Clickstream monthly files from 00-raw\n",
    "raw_dir = 'data/00-raw'\n",
    "pattern = os.path.join(raw_dir, 'clickstream-enwiki-*.tsv.gz')\n",
    "files = sorted(glob.glob(pattern))\n",
    "\n",
    "if not files:\n",
    "    raise FileNotFoundError(\"No clickstream files found in data/00-raw/. Run the setup cell above to download them.\")\n",
    "\n",
    "# Load and parse each monthly file; add month column\n",
    "dfs = []\n",
    "for path in files:\n",
    "    # e.g. clickstream-enwiki-2024-06.tsv.gz -> 2024-06\n",
    "    print(f\"Loading {path} ...\")\n",
    "    basename = os.path.basename(path)\n",
    "    month = basename.replace('clickstream-enwiki-', '').replace('.tsv.gz', '')\n",
    "    df = pd.read_csv(\n",
    "        path, sep='\\t', compression='gzip',\n",
    "        header=None, names=['prev', 'curr', 'type', 'n'],\n",
    "        on_bad_lines='skip'\n",
    "    )\n",
    "    df['month'] = month\n",
    "    dfs.append(df)\n",
    "\n",
    "clickstream_raw = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(\"Shape:\", clickstream_raw.shape)\n",
    "print(\"Columns:\", clickstream_raw.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(clickstream_raw.head(10))\n",
    "print(\"\\nValue counts for 'prev' (top referrers):\")\n",
    "print(clickstream_raw['prev'].value_counts().head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing per column:\n",
      "prev     108\n",
      "curr     172\n",
      "type       0\n",
      "n         24\n",
      "month      0\n",
      "dtype: int64\n",
      "\n",
      "Rows with prev='other-search': 11725128\n",
      "\n",
      "Total clicks from search by month:\n",
      "     month  clicks_from_search\n",
      "0  2023-06        3.123554e+09\n",
      "1  2024-06        2.955755e+09\n",
      "2  2025-01        3.319773e+09\n",
      "\n",
      "Saved to data/02-processed/\n"
     ]
    }
   ],
   "source": [
    "# Missingness\n",
    "print(\"Missing per column:\")\n",
    "print(clickstream_raw.isnull().sum())\n",
    "# Clickstream dumps are complete for (prev, curr) pairs; no NA expected in prev, curr, n.\n",
    "\n",
    "# Restrict to search-originated traffic: prev == 'other-search' (external search engines)\n",
    "clickstream_search = clickstream_raw[clickstream_raw['prev'] == 'other-search'].copy()\n",
    "print(\"\\nRows with prev='other-search':\", len(clickstream_search))\n",
    "\n",
    "# Tidy: one row per (month, curr) with total clicks from search to that article\n",
    "clickstream_by_month = (\n",
    "    clickstream_search.groupby(['month', 'curr'], as_index=False)['n']\n",
    "    .sum()\n",
    "    .rename(columns={'n': 'clicks_from_search'})\n",
    ")\n",
    "# Total clicks from search per month (for trend analysis)\n",
    "monthly_totals = clickstream_by_month.groupby('month', as_index=False)['clicks_from_search'].sum()\n",
    "print(\"\\nTotal clicks from search by month:\")\n",
    "print(monthly_totals)\n",
    "\n",
    "# Save processed clickstream: search-only, by month and article\n",
    "clickstream_by_month.to_csv('data/02-processed/wikipedia_clickstream_search_by_month_article.csv', index=False)\n",
    "monthly_totals.to_csv('data/02-processed/wikipedia_clickstream_search_monthly_totals.csv', index=False)\n",
    "print(\"\\nSaved to data/02-processed/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Trends (Dataset #2)\n",
    "\n",
    "**Metrics and units:** Google Trends provides **interest over time**: a 0–100 index of relative search popularity for a given keyword in a given time window (no absolute volume). We use **date** (or weekly buckets) and **interest** (0–100) per keyword. This tells us whether people were still searching for the same topics across 2023–2025.\n",
    "\n",
    "**Relevance:** If Trends interest stays stable or rises while Wikipedia clickstream counts from search drop, that supports the hypothesis that more searches are “zero-click” (answers consumed in-search rather than by clicking through to Wikipedia).\n",
    "\n",
    "**Concerns:** Relative scale only; geographic and category filters affect results; API/rate limits may apply when using `pytrends`; data is sampled.\n",
    "\n",
    "**Obtaining the data:** We use the `pytrends` library to fetch interest over time for chosen keywords (e.g. \"Quantum mechanics\", \"Climate change\"). Below we load or fetch a small example and tidy it; for the full analysis we will fetch multiple keywords and align by month with the clickstream data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (218, 4)\n",
      "        date            keyword  interest    month\n",
      "0 2023-01-01  Quantum mechanics         0  2023-01\n",
      "1 2023-01-08  Quantum mechanics         0  2023-01\n",
      "2 2023-01-15  Quantum mechanics         0  2023-01\n",
      "3 2023-01-22  Quantum mechanics         0  2023-01\n",
      "4 2023-01-29  Quantum mechanics         0  2023-01\n",
      "5 2023-02-05  Quantum mechanics         0  2023-02\n",
      "6 2023-02-12  Quantum mechanics         0  2023-02\n",
      "7 2023-02-19  Quantum mechanics         0  2023-02\n",
      "8 2023-02-26  Quantum mechanics         0  2023-02\n",
      "9 2023-03-05  Quantum mechanics         0  2023-03\n",
      "\n",
      "Saved to data/02-processed/google_trends_interest_long.csv\n"
     ]
    }
   ],
   "source": [
    "# Google Trends: fetch via pytrends (install with: pip install pytrends)\n",
    "# If pytrends is not available, we can load a pre-downloaded CSV from 00-raw instead.\n",
    "\n",
    "try:\n",
    "    from pytrends.request import TrendReq\n",
    "    pytrends_available = True\n",
    "except ImportError:\n",
    "    pytrends_available = False\n",
    "    print(\"pytrends not installed. Install with: pip install pytrends\")\n",
    "    print(\"Alternatively, export CSVs from https://trends.google.com/ and load from data/00-raw/\")\n",
    "\n",
    "if pytrends_available:\n",
    "    pt = TrendReq(hl='en-US', tz=360)\n",
    "    # Example: interest over time for a few informational keywords (2023-01-01 to 2025-02-01)\n",
    "    keywords = [\"Quantum mechanics\", \"Climate change\"]\n",
    "    pt.build_payload(keywords, timeframe='2023-01-01 2025-02-01', geo='')\n",
    "    trends_df = pt.interest_over_time()\n",
    "    if 'isPartial' in trends_df.columns:\n",
    "        trends_df = trends_df.drop(columns=['isPartial'])\n",
    "    trends_df = trends_df.reset_index()\n",
    "    trends_df = trends_df.rename(columns={'date': 'date'})\n",
    "    # Tidy: long form (date, keyword, interest)\n",
    "    trends_long = trends_df.melt(id_vars=['date'], var_name='keyword', value_name='interest')\n",
    "    trends_long['month'] = trends_long['date'].dt.to_period('M').astype(str)\n",
    "    print(\"Shape:\", trends_long.shape)\n",
    "    print(trends_long.head(10))\n",
    "    # Save processed Trends data\n",
    "    trends_long.to_csv('data/02-processed/google_trends_interest_long.csv', index=False)\n",
    "    print(\"\\nSaved to data/02-processed/google_trends_interest_long.csv\")\n",
    "else:\n",
    "    # Placeholder: create minimal structure so rest of notebook can run\n",
    "    import pandas as pd\n",
    "    trends_long = pd.DataFrame({'month': [], 'keyword': [], 'interest': []})\n",
    "    print(\"Google Trends data will be loaded from CSV or fetched once pytrends is installed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the two datasets\n",
    "\n",
    "We align Wikipedia Clickstream and Google Trends by **month**. For each month we have (1) total (or topic-specific) clicks from search to Wikipedia from the clickstream data, and (2) average or total interest per keyword from Trends. We can then compute a simple ratio or normalized metric (e.g. clicks per unit of interest) and compare pre–AI Overviews (e.g. 2023) vs post–AI Overviews (2024–2025). Below we load the processed files and join on `month` for illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset (by month):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>wiki_clicks_from_search</th>\n",
       "      <th>interest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-06</td>\n",
       "      <td>3.123554e+09</td>\n",
       "      <td>1.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2024-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2024-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2024-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2024-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2024-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2024-06</td>\n",
       "      <td>2.955755e+09</td>\n",
       "      <td>1.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2024-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2024-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2024-09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2024-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2024-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2024-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2025-01</td>\n",
       "      <td>3.319773e+09</td>\n",
       "      <td>2.125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      month  wiki_clicks_from_search  interest\n",
       "0   2023-01                      NaN     1.900\n",
       "1   2023-02                      NaN     2.125\n",
       "2   2023-03                      NaN     2.500\n",
       "3   2023-04                      NaN     8.300\n",
       "4   2023-05                      NaN     2.375\n",
       "5   2023-06             3.123554e+09     1.750\n",
       "6   2023-07                      NaN     1.600\n",
       "7   2023-08                      NaN     1.625\n",
       "8   2023-09                      NaN     2.000\n",
       "9   2023-10                      NaN     2.000\n",
       "10  2023-11                      NaN     2.000\n",
       "11  2023-12                      NaN     1.600\n",
       "12  2024-01                      NaN     1.875\n",
       "13  2024-02                      NaN     2.125\n",
       "14  2024-03                      NaN     2.200\n",
       "15  2024-04                      NaN    14.500\n",
       "16  2024-05                      NaN     2.375\n",
       "17  2024-06             2.955755e+09     1.600\n",
       "18  2024-07                      NaN     1.500\n",
       "19  2024-08                      NaN     1.500\n",
       "20  2024-09                      NaN     1.900\n",
       "21  2024-10                      NaN     2.125\n",
       "22  2024-11                      NaN     2.375\n",
       "23  2024-12                      NaN     1.600\n",
       "24  2025-01             3.319773e+09     2.125"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load processed data and align by month\n",
    "monthly_totals = pd.read_csv('data/02-processed/wikipedia_clickstream_search_monthly_totals.csv')\n",
    "monthly_totals = monthly_totals.rename(columns={'clicks_from_search': 'wiki_clicks_from_search'})\n",
    "\n",
    "# If we have Trends data, average interest by month and join\n",
    "trends_path = 'data/02-processed/google_trends_interest_long.csv'\n",
    "if os.path.exists(trends_path) and os.path.getsize(trends_path) > 0:\n",
    "    trends = pd.read_csv(trends_path)\n",
    "    trends['date'] = pd.to_datetime(trends['date'])\n",
    "    trends['month'] = trends['date'].dt.to_period('M').astype(str)\n",
    "    trends_monthly = trends.groupby('month', as_index=False)['interest'].mean()\n",
    "    combined = monthly_totals.merge(trends_monthly, on='month', how='outer')\n",
    "    print(\"Combined dataset (by month):\")\n",
    "    display(combined)\n",
    "else:\n",
    "    print(\"Wikipedia monthly totals (Trends will be joined when available):\")\n",
    "    display(monthly_totals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: Keep the contents of this cell. For each item on the checklist\n",
    "-  put an X there if you've considered the item\n",
    "-  IF THE ITEM IS RELEVANT place a short paragraph after the checklist item discussing the issue.\n",
    "  \n",
    "Items on this checklist are meant to provoke discussion among good-faith actors who take their ethical responsibilities seriously. Your teams will document these discussions and decisions for posterity using this section.  You don't have to solve these problems, you just have to acknowledge any potential harm no matter how unlikely.\n",
    "\n",
    "Here is a [list of real world examples](https://deon.drivendata.org/examples/) for each item in the checklist that can refer to.\n",
    "\n",
    "[![Deon badge](https://img.shields.io/badge/ethics%20checklist-deon-brightgreen.svg?style=popout-square)](http://deon.drivendata.org/)\n",
    "\n",
    "### A. Data Collection\n",
    " - [X] **A.1 Informed consent**: If there are human subjects, have they given informed consent, where subjects affirmatively opt-in and have a clear understanding of the data uses to which they consent?\n",
    "\n",
    "    >Our datasets present an interesting informed consent situation. While Wikipedia Clickstream and Google Trends data contain no personally identifiable information, the clickstream data originates from real users' browsing behavior — people who navigated from Google to Wikipedia without knowing their behavior would be aggregated and used in research. Wikimedia publishes this data openly under their privacy policy, which users implicitly accept, but true affirmative opt-in consent was never obtained. We consider this an acceptable tradeoff given the data is fully anonymized and aggregated before we ever access it, meaning no individual can be identified. Additionally, there is a reasonable argument that most internet users today have a general awareness that their searches are not private — Google Trends itself is a widely used public tool that many people interact with directly. This awareness has arguably been further reinforced culturally through true crime media, where high-profile cases like Casey Anthony and the Idaho student murders brought mainstream attention to the fact that search histories can be subpoenaed and used as evidence. Whether or not users formally consented, there is broad public understanding that online search behavior leaves a traceable record. Google Trends data raises fewer concerns as it is a deliberately public-facing tool that Google explicitly designed for research and analysis purposes.\n",
    "\n",
    " - [X] **A.2 Collection bias**: Have we considered sources of bias that could be introduced during data collection and survey design and taken steps to mitigate those?\n",
    "\n",
    "    >Our analysis is intentionally scoped to English-language searches using English Wikipedia Clickstream data, which allows for a more controlled comparison while acknowledging this limits generalizability to non-English-speaking populations. Within this scope, our most significant source of collection bias stems from using Wikipedia Clickstream as a proxy for zero-click behavior. Wikipedia attracts a specific type of user — generally educated, English-speaking, and seeking encyclopedic information — meaning our analysis inherently reflects only a narrow subset of informational queries. Someone searching \"how do I renew my license\" or \"best pizza near me\" will never land on Wikipedia regardless of whether AI Overviews exist, so our measure of zero-click behavior is really only valid for a particular class of curiosity-driven informational queries. Additionally, Google Trends normalizes all data to a scale of 0-100 relative to peak interest rather than showing absolute search volume, meaning if overall search volume grew significantly after AI Overviews launched, Trends would not capture that, potentially masking the true scale of behavioral change. Finally, we must be careful about which Wikipedia articles we select, as certain topics introduce noise that has nothing to do with AI Overviews — for example, a celebrity death, a breaking news event, or a viral moment during 2023-2025 would cause an artificial spike or crash in clicks that would contaminate our results. To mitigate this, we will intentionally exclude any topics that experienced major real-world events during our study period and instead restrict our selection to stable, evergreen topics such as major companies, living celebrities who did not die between 2023-2025, and well-established historical events. These topics have consistent baseline search interest, making them a much cleaner signal for detecting whether AI Overviews are intercepting traffic.\n",
    "\n",
    " - [X] **A.3 Limit PII exposure**: Have we considered ways to minimize exposure of personally identifiable information (PII) for example through anonymization or not collecting information that isn't relevant for analysis?\n",
    "\n",
    "    >Our datasets contain no personally identifiable information at any level. Wikipedia Clickstream data is purely aggregate click counts with no user or location data attached. While Google Trends allows filtering by region, we will not be using location-based filtering in our analysis, meaning we never interact with even aggregate location data. There is no PII exposure risk in this project.\n",
    "\n",
    " - [X] **A.4 Downstream bias mitigation**: Have we considered ways to enable testing downstream results for biased outcomes (e.g., collecting data on protected group status like race or gender)?\n",
    "\n",
    "### B. Data Storage\n",
    " - [X] **B.1 Data security**: Do we have a plan to protect and secure data (e.g., encryption at rest and in transit, access controls on internal users and third parties, access logs, and up-to-date software)?\n",
    " - [X] **B.2 Right to be forgotten**: Do we have a mechanism through which an individual can request their personal information be removed?\n",
    " - [X] **B.3 Data retention plan**: Is there a schedule or plan to delete the data after it is no longer needed?\n",
    "    >Data files will be deleted from local machines after the project concludes in March 2026.\n",
    "\n",
    "### C. Analysis\n",
    " - [X] **C.1 Missing perspectives**: Have we sought to address blindspots in the analysis through engagement with relevant stakeholders (e.g., checking assumptions and discussing implications with affected communities and subject matter experts)?\n",
    "    >We did not formally engage with outside stakeholders or domain experts during the development of this project. The perspectives most notably absent are those of anyone who creates content for the web and depends on organic search traffic to reach an audience — this includes bloggers, journalists, writers, photographers, UX designers, and anyone who has built a website whether through custom development or platforms like Wix or Squarespace. These are the people most directly and economically harmed by the trends we are studying, as reduced organic clicks means fewer people ever reach their work. SEO professionals who track click-through rates professionally would also have been valuable consultants for validating our methodology. We acknowledge these missing perspectives as a limitation of our analysis, and note that our conclusions should be interpreted with the understanding that they were not reviewed by those with firsthand experience of the phenomenon we are studying.\n",
    "\n",
    " - [X] **C.2 Dataset bias**: Have we examined the data for possible sources of bias and taken steps to mitigate or address these biases (e.g., stereotype perpetuation, confirmation bias, imbalanced classes, or omitted confounding variables)?\n",
    "\n",
    "    >Our most significant dataset bias is that we are exclusively using English Wikipedia Clickstream data and English-language Google Trends queries, meaning our findings only reflect the behavior of English-speaking users. This is an intentional scope decision rather than an oversight, but it does mean our conclusions cannot be generalized to non-English-speaking populations who may interact with AI Overviews very differently. Confirmation bias is also worth acknowledging — given that our hypothesis already predicts that AI Overviews will reduce organic clicks, we must be careful not to selectively interpret our findings in ways that support that conclusion. We will mitigate this by letting the statistical tests drive our conclusions rather than cherry-picking time periods or topics that support our hypothesis. There are also meaningful confounding variables that could explain drops in Wikipedia clicks that have nothing to do with AI Overviews. One notable example is the growing awareness around source reliability and media literacy — as more people learn about evaluating credible sources, Wikipedia's reputation as an anyone-can-edit platform may lead fewer people to click through to it regardless of whether AI Overviews exist. This cultural shift toward skepticism of Wikipedia as a reliable source could independently reduce click traffic in our study period and be mistakenly attributed to AI Overviews. We will attempt to account for confounding variables by comparing trends across multiple topics and looking for consistent patterns rather than relying on any single data point.\n",
    "\n",
    " - [X] **C.3 Honest representation**: Are our visualizations, summary statistics, and reports designed to honestly represent the underlying data?\n",
    "    >We are committed to honestly representing our data in all visualizations and summary statistics. This means we will not manipulate axis scales, cherry-pick time ranges, or present results in misleading ways. If our findings do not support our hypothesis, we will report that honestly rather than adjusting our analysis to fit our expectations.\n",
    "\n",
    " - [X] **C.4 Privacy in analysis**: Have we ensured that data with PII are not used or displayed unless necessary for the analysis?\n",
    " - [X] **C.5 Auditability**: Is the process of generating the analysis well documented and reproducible if we discover issues in the future?\n",
    "    >Our entire analysis will be documented and reproducible through our public GitHub repository. All data cleaning, wrangling, and statistical analysis will be conducted in Jupyter notebooks with clear explanations of each step. Anyone should be able to clone our repository, download the same publicly available datasets from Wikipedia Clickstream and Google Trends, and reproduce our exact results.\n",
    "\n",
    "### D. Modeling\n",
    " - [X] **D.1 Proxy discrimination**: Have we ensured that the model does not rely on variables or proxies for variables that are unfairly discriminatory?\n",
    " - [X] **D.2 Fairness across groups**: Have we tested model results for fairness with respect to different affected groups (e.g., tested for disparate error rates)?\n",
    " - [X] **D.3 Metric selection**: Have we considered the effects of optimizing for our defined metrics and considered additional metrics?\n",
    " - [X] **D.4 Explainability**: Can we explain in understandable terms a decision the model made in cases where a justification is needed?\n",
    " - [X] **D.5 Communicate limitations**: Have we communicated the shortcomings, limitations, and biases of the model to relevant stakeholders in ways that can be generally understood?\n",
    "\n",
    "### E. Deployment\n",
    " - [X] **E.1 Monitoring and evaluation**: Do we have a clear plan to monitor the model and its impacts after it is deployed (e.g., performance monitoring, regular audit of sample predictions, human review of high-stakes decisions, reviewing downstream impacts of errors or low-confidence decisions, testing for concept drift)?\n",
    " - [X] **E.2 Redress**: Have we discussed with our organization a plan for response if users are harmed by the results (e.g., how does the data science team evaluate these cases and update analysis and models to prevent future harm)?\n",
    " - [X] **E.3 Roll back**: Is there a way to turn off or roll back the model in production if necessary?\n",
    " - [X] **E.4 Unintended use**: Have we taken steps to identify and prevent unintended uses and abuse of the model and do we have a plan to monitor these once the model is deployed?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Communication:** Primarily via iMessage group chat; meet via FaceTime or in person as needed. Respond within 24 hours; notify in advance if unable to attend a meeting.\n",
    "- **Equal contribution:** Each member contributes equally across research, coding, writing, and editing; we rotate responsibilities.\n",
    "- **Tone and respect:** Blunt but polite; use \"I statements\" when giving feedback; assume criticism is well-intentioned.\n",
    "- **Task management:** Use GitHub for tasks and deadlines; assign fairly by strengths and availability. If struggling, notify within 48 hours to redistribute.\n",
    "- **Decision making:** Majority vote for major decisions; for urgent decisions when someone is unresponsive, available members can proceed and update afterward.\n",
    "- **Accountability:** If someone is not meeting expectations, address directly (e.g. via text) with one week to improve and specific deliverables; redistribute tasks as needed.\n",
    "- **Deadlines:** Internal deadlines 2–3 days before official course deadlines for review and revision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Special resources/training:** Statistical methods for pre/post AI Overviews comparison (e.g. t-tests, chi-square); clear visualizations for trends over time.\n",
    "\n",
    "| Meeting Date | Meeting Time | Completed Before Meeting | Discuss at Meeting |\n",
    "|--------------|--------------|--------------------------|-------------------|\n",
    "| 1/15 | 12 PM | NA | Determine best form of communication |\n",
    "| 1/23 | 10 AM | Brainstorm topics (All) | Brainstorm topics (All) |\n",
    "| 1/26 | 10 AM | Background research (Ali, Rodayna); Ethics draft (Sabine) | Ideal dataset(s) and ethics; draft proposal |\n",
    "| 2/4 | 10 AM | Edit, finalize, submit proposal (Maanav, Tessa); Search datasets (All) | Wrangling and analysis approaches; assign leads |\n",
    "| 2/4 | Before 11:59 PM | NA | **Turn in Project Proposal** |\n",
    "| 2/14 | 6 PM | Import & wrangle data (Maanav); Initial EDA with 3+ visualizations (Rodayna) | Review wrangling/EDA; analysis plan; data quality |\n",
    "| 2/14 | Before 11:59 PM | NA | **Turn in Data Checkpoint** |\n",
    "| 2/23 | 12 PM | Finalize wrangling/EDA (Maanav, Rodayna); Begin pre/post statistical analysis (Sabine, Tessa) | Edit analysis; preliminary results; check-in |\n",
    "| 3/4 | Before 11:59 PM | NA | **Turn in EDA Checkpoint** |\n",
    "| 3/9 | 12 PM | Finalize analysis and visualizations; Draft results/conclusion (Ali); Polish notebook (Maanav) | Review analysis; edit results/discussion; video outline |\n",
    "| 3/13 | 12 PM | Ethics updates (Sabine); Finalize written sections (Ali, Tessa); Record video segments (All) | Integrate video; final review |\n",
    "| 3/18 | Before 11:59 PM | NA | **Turn in Final Project & Video** |\n",
    "\n",
    "*Timeline updated for Data Checkpoint: data sources set (Wikipedia Clickstream + Google Trends); wrangling and combination by month in progress.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
